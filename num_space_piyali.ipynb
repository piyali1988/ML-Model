{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'than', 'now', 've', 'needn', 'above', \"shouldn't\", 're', 'very', 'to', 'because', 'between', \"you've\", 'same', 'don', 'all', 'weren', 'by', 's', 'an', 'once', 'ain', 'do', 'not', 'mightn', 'is', 'when', 'about', 'doesn', 'was', 'shouldn', 'have', 'what', \"hasn't\", \"won't\", 'doing', 'during', 'haven', 'both', 'but', 'didn', 'so', 'if', 'some', 'then', 'each', 'most', 'couldn', 'be', 'other', 'they', \"couldn't\", \"hadn't\", 'i', \"you're\", 'yours', 'being', 'were', 'wouldn', 'whom', 'their', \"needn't\", 'with', 'again', 'how', 'does', 'hadn', 'his', 'those', 'into', 'isn', 't', 'such', 'm', 'itself', 'before', 'of', 'her', 'hers', 'am', 'the', 'down', 'and', 'until', 'them', 'll', 'himself', 'him', 'against', 'hasn', 'ma', \"wouldn't\", 'where', 'herself', 'we', \"didn't\", 'themselves', 'he', 'its', \"isn't\", 'after', 'off', 'up', 'below', 'out', 'a', \"wasn't\", 'or', \"she's\", \"aren't\", \"you'll\", 'theirs', 'yourself', 'over', 'did', \"that'll\", 'that', 'here', 'aren', \"mustn't\", 'our', 'having', 'she', 'for', 'mustn', \"haven't\", 'had', \"mightn't\", 'can', \"doesn't\", 'should', 'this', 'no', 'few', 'own', \"don't\", 'only', 'shan', 'these', \"it's\", 'any', \"you'd\", 'while', 'o', 'which', 'nor', 'too', \"shan't\", 'more', 'why', 'just', 'y', 'there', 'through', 'myself', 'who', 'me', 'under', 'my', 'at', 'will', 'your', 'it', 'yourselves', 'been', 'has', 'on', 'in', 'won', 'wasn', \"weren't\", 'from', 'further', \"should've\", 'ourselves', 'are', 'd', 'ours', 'as', 'you'}\n"
     ]
    }
   ],
   "source": [
    "# importing stopwords for Python 3 vs Python 2.7 is different. Follow the format below to import.\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\das2\\\\Documents\\\\256_VS2.opf.csv']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Function to perform battery of programmatic nlp analyses including sentiment, LSA, LDA, turn_counts, POS, WC, UNQ\"\"\"\n",
    "import glob\n",
    "import datetime\n",
    "# getting current date and time\n",
    "d = datetime.datetime.today()\n",
    "path = 'C:\\\\Users\\\\das2\\\\Documents'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the output features and keep only input features like in input csv\n",
    "# 'df' holds individual csv files read\n",
    "# li is the list that holds all csv files that is read by 'df'\n",
    "li = []\n",
    "count = 0 \n",
    "for filename in all_files:\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='|', index_col=None, error_bad_lines=False, encoding='UTF-8')\n",
    "        df = df[['file_name', 'speaker_label', 'speech', 'speech_onset', 'speech_offset']]\n",
    "        li.append(df)\n",
    "    except:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>speaker_label</th>\n",
       "      <th>speech</th>\n",
       "      <th>speech_onset</th>\n",
       "      <th>speech_offset,,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>c_speech</td>\n",
       "      <td>no.</td>\n",
       "      <td>29</td>\n",
       "      <td>865,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>c_speech</td>\n",
       "      <td>my baby sleep.</td>\n",
       "      <td>3122</td>\n",
       "      <td>3894,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>c_speech</td>\n",
       "      <td>my baby sleep.</td>\n",
       "      <td>4563</td>\n",
       "      <td>6105,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>c_speech</td>\n",
       "      <td>### baby.</td>\n",
       "      <td>16778</td>\n",
       "      <td>17703,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>c_speech</td>\n",
       "      <td>give my milk.</td>\n",
       "      <td>38065</td>\n",
       "      <td>39273,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>p_speech</td>\n",
       "      <td>come here.</td>\n",
       "      <td>1822812</td>\n",
       "      <td>1823744,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>p_speech</td>\n",
       "      <td>here.</td>\n",
       "      <td>1825817</td>\n",
       "      <td>1826901,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>p_speech</td>\n",
       "      <td>come here ###</td>\n",
       "      <td>1828230</td>\n",
       "      <td>1829469,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>p_speech</td>\n",
       "      <td>no, D'Angelo@n.</td>\n",
       "      <td>1830297</td>\n",
       "      <td>1831628,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>256_VS2.opf</td>\n",
       "      <td>p_speech</td>\n",
       "      <td>---</td>\n",
       "      <td>1834601</td>\n",
       "      <td>1837107,,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>703 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name speaker_label           speech  speech_onset speech_offset,,\n",
       "0    256_VS2.opf      c_speech              no.            29           865,,\n",
       "1    256_VS2.opf      c_speech   my baby sleep.          3122          3894,,\n",
       "2    256_VS2.opf      c_speech   my baby sleep.          4563          6105,,\n",
       "3    256_VS2.opf      c_speech        ### baby.         16778         17703,,\n",
       "4    256_VS2.opf      c_speech    give my milk.         38065         39273,,\n",
       "..           ...           ...              ...           ...             ...\n",
       "698  256_VS2.opf      p_speech       come here.       1822812       1823744,,\n",
       "699  256_VS2.opf      p_speech            here.       1825817       1826901,,\n",
       "700  256_VS2.opf      p_speech    come here ###       1828230       1829469,,\n",
       "701  256_VS2.opf      p_speech  no, D'Angelo@n.       1830297        1831628,\n",
       "702  256_VS2.opf      p_speech              ---       1834601       1837107,,\n",
       "\n",
       "[703 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Dimension', 'Shape',\n",
    "       'Feature_Property', 'Location_Direction', 'Amount',\n",
    "       'Pattern_Comparison', 'Null']] = wb_es_ns_df[['Dimension', 'Shape',\n",
    "       'Feature_Property', 'Location_Direction', 'Amount',\n",
    "       'Pattern_Comparison', 'Null']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['256_VS2.opf', 'c_speech', 'no.', '29', '865'],\n",
       "       ['256_VS2.opf', 'c_speech', 'my baby sleep.', '3122', '3894'],\n",
       "       ['256_VS2.opf', 'c_speech', 'my baby sleep.', '4563', '6105'],\n",
       "       ...,\n",
       "       ['256_VS2.opf', 'p_speech', 'come here ###', '1828230', '1829469'],\n",
       "       ['256_VS2.opf', 'p_speech', \"no, D'Angelo@n.\", '1830297',\n",
       "        '1831628'],\n",
       "       ['256_VS2.opf', 'p_speech', '---', '1834601', '1837107']],\n",
       "      dtype='<U141')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separating out each line and processing as list of elements\n",
    "classes=[]\n",
    "\n",
    "for row in df.iterrows():\n",
    "    index, data = row\n",
    "    classes.append(data.tolist())\n",
    "\n",
    "classes=np.array(classes)\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract speech column only for the phrases and apply for df only because li is already list\n",
    "text_list = list(df['speech'])\n",
    "text_list = [text for text in text_list if type(text) != int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no.',\n",
       " 'my baby sleep.',\n",
       " 'my baby sleep.',\n",
       " '### baby.',\n",
       " 'give my milk.',\n",
       " 'Ma stop.',\n",
       " 'gone.',\n",
       " 'this cookie.',\n",
       " 'donut.',\n",
       " 'huh?',\n",
       " '###',\n",
       " '###',\n",
       " 'pizza.',\n",
       " '###',\n",
       " 'mine.',\n",
       " 'huh?',\n",
       " 'chicken.',\n",
       " '###',\n",
       " \"that's chip.\",\n",
       " 'Mommy got to like chip?',\n",
       " 'I like chip.',\n",
       " 'yeah.',\n",
       " 'steak.',\n",
       " 'yeah.',\n",
       " 'my baby.',\n",
       " 'thank+you.',\n",
       " 'hold it.',\n",
       " 'ice+cream.',\n",
       " 'ice+cream.',\n",
       " 'yep.',\n",
       " 'yep.',\n",
       " '### mine.',\n",
       " 'yep.',\n",
       " '###',\n",
       " 'yeah.',\n",
       " 'chocolate?',\n",
       " '###',\n",
       " 'yeah.',\n",
       " 'give my baby.',\n",
       " '### my baby sleep.',\n",
       " 'yep.',\n",
       " 'my baby -- sleep.',\n",
       " '### wake up.',\n",
       " '### gum.',\n",
       " 'ok.',\n",
       " 'ok.',\n",
       " '### baby.',\n",
       " 'my baby.',\n",
       " 'I change babber@.',\n",
       " 'no wet.',\n",
       " 'down+stairs.',\n",
       " 'down+stairs.',\n",
       " 'I read.',\n",
       " 'yeah.',\n",
       " 'gorilla.',\n",
       " 'yeah, monkey.',\n",
       " '###',\n",
       " 'bike.',\n",
       " 'hmm?',\n",
       " 'ball.',\n",
       " 'ball.',\n",
       " 'good+night elephant.',\n",
       " 'good+night lion.',\n",
       " 'good+night.',\n",
       " 'good+night ###',\n",
       " 'good+night, giraffe.',\n",
       " 'good+night, armadillo.',\n",
       " 'hmm?',\n",
       " '###',\n",
       " 'huh?',\n",
       " '###',\n",
       " 'hmm?',\n",
       " 'good+night, dear.',\n",
       " 'good+night.',\n",
       " '###',\n",
       " 'good+night.',\n",
       " '###',\n",
       " 'hmm?',\n",
       " 'good+night, zoo.',\n",
       " 'good+night, dear.',\n",
       " '###',\n",
       " '###',\n",
       " '###',\n",
       " 'yeah.',\n",
       " '###',\n",
       " 'move, Mommy.',\n",
       " 'excuse me.',\n",
       " '###',\n",
       " 'my baby.',\n",
       " 'big baby.',\n",
       " '###',\n",
       " '###',\n",
       " 'five?',\n",
       " '###',\n",
       " 'a@l b@l c@l d@l.',\n",
       " 'eight.',\n",
       " 'two.',\n",
       " 'move, Mommy.',\n",
       " 'excuse me.',\n",
       " 'nine.',\n",
       " 'seven.',\n",
       " '###',\n",
       " 'four.',\n",
       " 'six.',\n",
       " 'count.',\n",
       " '### my ball.',\n",
       " 'one.',\n",
       " \"don't touch it.\",\n",
       " '###',\n",
       " '###',\n",
       " 'my baby.',\n",
       " '### my ###',\n",
       " 'huh?',\n",
       " 'meow.',\n",
       " '---',\n",
       " 'duck.',\n",
       " 'duck.',\n",
       " '---',\n",
       " '---',\n",
       " '###',\n",
       " 'yep.',\n",
       " '---',\n",
       " 'moo -- moo.',\n",
       " '---',\n",
       " '###',\n",
       " 'ok.',\n",
       " 'hooray.',\n",
       " 'happy.',\n",
       " '---',\n",
       " '###',\n",
       " '###',\n",
       " 'bee+bo?',\n",
       " 'no.',\n",
       " 'go back.',\n",
       " 'go back.',\n",
       " '###',\n",
       " 'go.',\n",
       " 'move, Mommy.',\n",
       " '###',\n",
       " '###',\n",
       " '###',\n",
       " 'bye+bye Mommy.',\n",
       " '###',\n",
       " 'good+night.',\n",
       " 'good+night, elephant.',\n",
       " 'good+night, lion.',\n",
       " 'good+night hyena.',\n",
       " 'good+night, giraffe.',\n",
       " 'good+night armadillo.',\n",
       " 'good+night, dear.',\n",
       " 'good+night.',\n",
       " 'good+night.',\n",
       " '### eye+ball.',\n",
       " 'good+night, zoo.',\n",
       " 'good+night, dear.',\n",
       " 'sleep.',\n",
       " 'sleeping.',\n",
       " 'everybody sleeping.',\n",
       " 'everybody tired.',\n",
       " 'yeah, my baby tired.',\n",
       " 'yeah.',\n",
       " 'my baby woke up.',\n",
       " 'yeah.',\n",
       " '### go.',\n",
       " 'to store.',\n",
       " '###',\n",
       " '###',\n",
       " '###',\n",
       " '###',\n",
       " \"I'm scary.\",\n",
       " 'lion scare.',\n",
       " 'lion.',\n",
       " 'oh scary.',\n",
       " 'lion crazy ###',\n",
       " 'hold it.',\n",
       " '###',\n",
       " '###',\n",
       " '###',\n",
       " '###',\n",
       " 'no.',\n",
       " 'no.',\n",
       " 'no.',\n",
       " 'no.',\n",
       " 'no.',\n",
       " 'no.',\n",
       " 'all done.',\n",
       " '###',\n",
       " 'chill?',\n",
       " 'my baby?',\n",
       " 'my baby ###',\n",
       " 'ten finger.',\n",
       " 'ten toe right here.',\n",
       " 'ten little toes.',\n",
       " '### scary.',\n",
       " 'ok.',\n",
       " 'I scary.',\n",
       " 'the+end.',\n",
       " '###',\n",
       " '---',\n",
       " 'your baby sleep?',\n",
       " 'how you feeding your baby when your baby sleep?',\n",
       " \"what's your baby name?\",\n",
       " \"what's your baby name?\",\n",
       " 'can I see the baby?',\n",
       " 'can I see the baby?',\n",
       " 'the baby?',\n",
       " 'the baby sleep.',\n",
       " 'let me hold the baby like this.',\n",
       " 'give baby some milk?',\n",
       " 'where the milk?',\n",
       " 'show me the milk.',\n",
       " \"no, that's zero.\",\n",
       " 'say zero.',\n",
       " 'say zero.',\n",
       " 'orange?',\n",
       " 'you want to give the baby some orange?',\n",
       " 'mmm.',\n",
       " \"it's good?\",\n",
       " 'can you say abcs?',\n",
       " 'abcs?',\n",
       " 'stack them on top.',\n",
       " 'here.',\n",
       " 'you want me to gone?',\n",
       " \"I can't help you?\",\n",
       " 'can I help you?',\n",
       " \"D'Angelo@n, can Mommy help you?\",\n",
       " \"what's a --\",\n",
       " \"that's a cookie?\",\n",
       " 'no, this is a cookie.',\n",
       " \"that's a donut.\",\n",
       " 'can you say donut?',\n",
       " \"here's the cookie.\",\n",
       " 'what you say?',\n",
       " \"no, don't eat that.\",\n",
       " 'what you say?',\n",
       " 'what you say?',\n",
       " 'say thank+you.',\n",
       " 'can I get the cookie?',\n",
       " 'thank+you.',\n",
       " 'mmm.',\n",
       " \"what's this?\",\n",
       " \"what's this?\",\n",
       " \"hmm, what's this?\",\n",
       " 'what is it?',\n",
       " 'pizza?',\n",
       " \"what's this?\",\n",
       " \"what's this?\",\n",
       " \"what's that?\",\n",
       " \"what's this?\",\n",
       " 'what is it?',\n",
       " 'chicken.',\n",
       " 'hmm.',\n",
       " \"what's this?\",\n",
       " 'apple.',\n",
       " 'chips.',\n",
       " 'om num[x4].',\n",
       " 'mmm.',\n",
       " \"that's chips.\",\n",
       " 'you like chips?',\n",
       " 'Mommy got to love chips.',\n",
       " 'you like chips?',\n",
       " \"that's good.\",\n",
       " 'what else you like?',\n",
       " 'steak?',\n",
       " 'you like steak?',\n",
       " 'you kind+of expensive.',\n",
       " 'oh, let me get your baby.',\n",
       " \"you're welcome.\",\n",
       " 'there you go.',\n",
       " 'get your baby.',\n",
       " 'you going to feed the baby?',\n",
       " 'pizza.',\n",
       " 'your baby like pizza?',\n",
       " 'mmm.',\n",
       " 'mmm.',\n",
       " \"ok, I'm gong to hold the baby.\",\n",
       " 'feed the baby?',\n",
       " 'ok.',\n",
       " 'there you go.',\n",
       " 'what else the baby like?',\n",
       " 'hmm?',\n",
       " 'the baby like ice+cream?',\n",
       " 'you going to give the baby some ice+cream?',\n",
       " 'give the baby ice+cream.',\n",
       " \"that's burritos and --\",\n",
       " 'mmm.',\n",
       " 'no.',\n",
       " 'what you say?',\n",
       " \"look, let's make a sandwich.\",\n",
       " 'look, let me seat your baby right here, ok?',\n",
       " 'look.',\n",
       " 'bread.',\n",
       " 'and lettuce.',\n",
       " 'oh bacon.',\n",
       " 'you want to put bacon on the sandwich?',\n",
       " 'you like bacon on your sandwich?',\n",
       " 'huh?',\n",
       " 'sandwich.',\n",
       " 'oh, we got some chocolate for dessert.',\n",
       " 'yeah, chocolate.',\n",
       " 'you put it all together?',\n",
       " 'yeah?',\n",
       " \"what's this book?\",\n",
       " \"what's that?\",\n",
       " 'ok, you want your baby back?',\n",
       " '---',\n",
       " 'your baby sleep?',\n",
       " 'your baby going to eat and sleep?',\n",
       " 'ok.',\n",
       " 'your baby sleep?',\n",
       " 'when your baby going to wake up?',\n",
       " 'you going to wake your baby up right now?',\n",
       " 'ok, wake your baby up.',\n",
       " \"don't shake it.\",\n",
       " 'oh, man.',\n",
       " \"don't shake the baby.\",\n",
       " \"no, the baby don't eat gum.\",\n",
       " 'ok.',\n",
       " \"what's the baby name again?\",\n",
       " \"you're going to show baby?\",\n",
       " \"you're going to kill baby.\",\n",
       " \"no, you -- you can't change the babber@.\",\n",
       " 'no -- yeah.',\n",
       " 'he not wet.',\n",
       " 'where his shoes?',\n",
       " 'they where?',\n",
       " 'they down+stairs?',\n",
       " 'oh my gosh.',\n",
       " 'you want me read it to you and your baby?',\n",
       " 'ok, come over here.',\n",
       " 'read.',\n",
       " 'put your baby right here.',\n",
       " 'hold your baby.',\n",
       " 'yeah.',\n",
       " 'Good+Night_Gorilla.',\n",
       " 'good+night, gorilla.',\n",
       " \"what's this?\",\n",
       " 'a monkey.',\n",
       " \"what's this?\",\n",
       " 'bike.',\n",
       " \"ooh, what's this?\",\n",
       " \"what's that?\",\n",
       " 'look at --',\n",
       " 'and animals.',\n",
       " 'ball?',\n",
       " 'good+night, elephant.',\n",
       " 'good+night, lion.',\n",
       " 'good+night -- hyena.',\n",
       " 'say good+night, giraffe.',\n",
       " 'good+night, armadillo.',\n",
       " 'look at all the animals.',\n",
       " 'they following him.',\n",
       " 'yeah, he following him to his house.',\n",
       " 'look.',\n",
       " 'how they fit in his house?',\n",
       " 'he got big house?',\n",
       " 'yeah?',\n",
       " 'oh look, everybody in a bed.',\n",
       " 'good+night, dear.',\n",
       " '&nuh night.',\n",
       " 'good+night.',\n",
       " 'good+night.',\n",
       " 'good+night.',\n",
       " 'look at the eye+balls.',\n",
       " 'look.',\n",
       " 'somebody ###',\n",
       " 'who is that?',\n",
       " 'who is that?',\n",
       " 'ah!',\n",
       " 'oh, she woke up.',\n",
       " 'she took them all back to the zoo.',\n",
       " 'good+night, zoo.',\n",
       " 'good+night, dear.',\n",
       " 'good+night.',\n",
       " 'good+night, gorilla.',\n",
       " 'yay.',\n",
       " 'good job.',\n",
       " 'see the gorilla?',\n",
       " \"look, let's match them.\",\n",
       " \"it's excuse me.\",\n",
       " 'say excuse me.',\n",
       " 'excuse me.',\n",
       " \"here, let's put these where they go.\",\n",
       " 'look.',\n",
       " 'yeah, your baby.',\n",
       " 'you got a big baby.',\n",
       " 'yup.',\n",
       " \"look, let's do this.\",\n",
       " '###',\n",
       " 'yep.',\n",
       " 'zero.',\n",
       " \"where's number one?\",\n",
       " 'pick up number one.',\n",
       " 'which one is over here?',\n",
       " \"that's five.\",\n",
       " 'no.',\n",
       " \"that one don't go there.\",\n",
       " 'eight.',\n",
       " \"where's eight go?\",\n",
       " 'look.',\n",
       " 'eight goes right here.',\n",
       " 'eight frogs.',\n",
       " \"let's count.\",\n",
       " 'one.',\n",
       " 'three.',\n",
       " 'say excuse me.',\n",
       " 'ready?',\n",
       " 'turn.',\n",
       " \"look, where's nine go?\",\n",
       " 'nine.',\n",
       " 'nine.',\n",
       " 'seven.',\n",
       " '###',\n",
       " 'one.',\n",
       " \"that's one.\",\n",
       " \"look, that's one.\",\n",
       " 'one snake.',\n",
       " 'two.',\n",
       " 'four.',\n",
       " 'say four.',\n",
       " 'four.',\n",
       " 'look at this one.',\n",
       " 'three.',\n",
       " 'six.',\n",
       " 'say six.',\n",
       " 'and five.',\n",
       " 'can you count?',\n",
       " 'count them.',\n",
       " 'one.',\n",
       " 'ok, count.',\n",
       " 'one.',\n",
       " 'one.',\n",
       " 'two.',\n",
       " \"ok, I won't touch it.\",\n",
       " 'two.',\n",
       " 'three.',\n",
       " \"ok, here's your baby.\",\n",
       " 'say taco.',\n",
       " 'your baby eat a+lot.',\n",
       " 'your baby like tacos too?',\n",
       " 'mmm.',\n",
       " 'you like -- you like tacos?',\n",
       " 'no?',\n",
       " 'look at the cat.',\n",
       " 'meow.',\n",
       " 'meow.',\n",
       " 'look.',\n",
       " 'baa.',\n",
       " 'say baa.',\n",
       " 'sing Old+Mac+Donald.',\n",
       " '---',\n",
       " 'a which one?',\n",
       " 'duck?',\n",
       " '---',\n",
       " 'oh sing it.',\n",
       " '---',\n",
       " 'a cow?',\n",
       " 'a cow.',\n",
       " '---',\n",
       " 'say moo -- moo.',\n",
       " '---',\n",
       " 'you feeding the baby again?',\n",
       " \"ooh, this baby's going to be big.\",\n",
       " 'moo.',\n",
       " 'moo.',\n",
       " 'moo -- moo.',\n",
       " '---',\n",
       " 'you know other songs?',\n",
       " \"want to sing If_You're_Happy_And_You_Know_It?\",\n",
       " '---',\n",
       " 'just leave it here.',\n",
       " '---',\n",
       " \"I'll start.\",\n",
       " '---',\n",
       " 'hooray.',\n",
       " '---',\n",
       " 'hooray.',\n",
       " '---',\n",
       " 'over there?',\n",
       " '---',\n",
       " 'you want to read that one?',\n",
       " 'ok.',\n",
       " 'come on.',\n",
       " 'get your baby.',\n",
       " 'get your baby.',\n",
       " 'get your baby so we can read the book.',\n",
       " 'sit the baby in your lap.',\n",
       " 'there we go.',\n",
       " 'Belly+Button_Book.',\n",
       " 'look at the belly+button.',\n",
       " 'look at the belly+button.',\n",
       " 'this tiny hippopotamus has something small to say.',\n",
       " \"and if we're very quiet she'll say it right away.\",\n",
       " 'listen.',\n",
       " 'bee+bo.',\n",
       " \"you might not know what bee+bo means or maybe you've forgotten.\",\n",
       " \"it's just a tiny hippo way of saying --\",\n",
       " 'belly+button.',\n",
       " 'you have to read it --',\n",
       " 'from left to right.',\n",
       " \"it's excuse me.\",\n",
       " 'we hippos love our belly+bs.',\n",
       " \"they're round and cute and funny.\",\n",
       " \"and there's a place we take them to when summer days are sunny.\",\n",
       " \"you don't want to read no more?\",\n",
       " 'come on.',\n",
       " \"let's finish.\",\n",
       " 'look at all the hippos with a belly+button each.',\n",
       " 'look.',\n",
       " 'what is it?',\n",
       " \"where's your belly+button?\",\n",
       " \"you don't know?\",\n",
       " \"let's see.\",\n",
       " \"it's right there.\",\n",
       " \"it's belly+button beach.\",\n",
       " 'where tons of hippos stand around in bathing+suits too little because they hope you wil admire the button on their middle.',\n",
       " \"we don't do much throughout the day.\",\n",
       " \"that's how we like it best.\",\n",
       " 'we nibble grapes.',\n",
       " 'we watch the waves.',\n",
       " 'we take a little rest.',\n",
       " 'we always love to get balloons -- and -- I know why.',\n",
       " 'do you?',\n",
       " 'because we like to think balloons have belly+buttons too.',\n",
       " \"it's almost over.\",\n",
       " 'soon after dark upon the beach we sing our favorite song.',\n",
       " \"and if you're feeling in the mood we hope you'll sing along.\",\n",
       " \"belly belly+button, you're oh so fine.\",\n",
       " \"oh belly+button, I'm so happy you're mine.\",\n",
       " \"we sing this song on summer nights or when it's hot outside, but never in cold winter+time when belly+buttons hide.\",\n",
       " 'no, never in cold winter+time when belly+buttons hide.',\n",
       " 'yay.',\n",
       " 'you like that one?',\n",
       " 'you want to read that one again?',\n",
       " 'ok.',\n",
       " 'Good+Night_Gorilla.',\n",
       " 'good+night, gorilla.',\n",
       " 'good+night, elephant.',\n",
       " 'good+night, lion.',\n",
       " 'good+night, hyena.',\n",
       " 'good+night, giraffe.',\n",
       " 'good+night, armadillo.',\n",
       " 'good+night, dear.',\n",
       " 'good+night.',\n",
       " 'good+night.',\n",
       " 'good+night.',\n",
       " 'the glowing eye+balls.',\n",
       " 'good+night, zoo.',\n",
       " 'good+night, dear.',\n",
       " 'good+night.',\n",
       " 'good+night, gorilla.',\n",
       " 'yep.',\n",
       " \"everybody's sleeping.\",\n",
       " \"yep, everybody's sleeping.\",\n",
       " \"everybody's tired.\",\n",
       " 'no -- no.',\n",
       " '###',\n",
       " \"yep, everybody's tired.\",\n",
       " 'are you tired?',\n",
       " 'are you tired?',\n",
       " 'your baby tired?',\n",
       " 'I thought you said your baby just woke up.',\n",
       " 'your baby woke up?',\n",
       " 'what you and your baby going to do today?',\n",
       " '### go where?',\n",
       " 'to the store?',\n",
       " 'you and your baby go to the store?',\n",
       " 'what you going get from the store?',\n",
       " '---',\n",
       " 'ah!',\n",
       " 'oh my goodness.',\n",
       " '---',\n",
       " 'put them in there.',\n",
       " 'say bye+bye to your baby.',\n",
       " '---',\n",
       " 'can you put them in the bag?',\n",
       " \"let's put them in there.\",\n",
       " \"put that in your mouth or you're going to throw it up.\",\n",
       " '###',\n",
       " \"what's that?\",\n",
       " 'nope, uhuh.',\n",
       " 'we going to chat right away.',\n",
       " 'you sorry?',\n",
       " \"it's scary?\",\n",
       " 'you scared?',\n",
       " 'lion not scared of lion.',\n",
       " 'lion scared of lion?',\n",
       " 'why everybody crazy?',\n",
       " '---',\n",
       " \"come on, let's read the books.\",\n",
       " 'Ten_Little_Fingers_And_Ten_Little_Toes.',\n",
       " 'there was one little baby who was born far away and another who was born on the very next day.',\n",
       " 'both of these babies as everyone knows, had ten little and ten little toes.',\n",
       " 'there was one little baby who was born in a town and another who was wrapped in an -- eiderdown -- eiderdown?',\n",
       " 'and both of these babies as everyone knows -- had ten little fingers and ten little toes.',\n",
       " 'go like this.',\n",
       " 'like this with your toes.',\n",
       " 'yeah, wiggle the toes.',\n",
       " 'wiggle the finger.',\n",
       " 'there was one little baby who was born in the hills -- and another who suffered from sneezes and chills.',\n",
       " 'and both of these babies as everyone knows --',\n",
       " 'look.',\n",
       " 'had ten little fingers and ten little toes.',\n",
       " 'there was one little baby who was born on the ice -- and another in a tent who was just as nice.',\n",
       " 'and both of these babies as everyone knows --',\n",
       " 'had ten little fingers -- fingers and ten little toes.',\n",
       " 'but the next baby -- born was truly divine.',\n",
       " 'a sweet little child who was mine, all mine.',\n",
       " 'and this little baby as everyone knows -- had ten little fingers -- and ten little toes -- and three little kisses -- on the tip of its nose.',\n",
       " \"where's your nose?\",\n",
       " 'mouth.',\n",
       " \"where's your toes?\",\n",
       " 'and your fingers?',\n",
       " '###',\n",
       " 'crocodile smiles.',\n",
       " 'say cheese.',\n",
       " 'cheese.',\n",
       " 'leopard growls.',\n",
       " 'raar.',\n",
       " \"oh, you're scared.\",\n",
       " \"ok, I'm sorry.\",\n",
       " \"it's fake.\",\n",
       " 'look.',\n",
       " \"look, it's fake.\",\n",
       " 'polar+bear yawns.',\n",
       " 'are you scared?',\n",
       " 'yeah?',\n",
       " 'aw.',\n",
       " \"you don't like that one.\",\n",
       " \"let's see what else.\",\n",
       " 'look.',\n",
       " \"I'm sorry.\",\n",
       " \"look, it's fake.\",\n",
       " 'orangutan laughs.',\n",
       " 'see, look.',\n",
       " 'and the shark chomp.',\n",
       " \"no, you don't like this one?\",\n",
       " 'why?',\n",
       " \"look, it's fake.\",\n",
       " 'it just pictures.',\n",
       " \"it's just pictures.\",\n",
       " 'the+end.',\n",
       " 'say the+end.',\n",
       " \"we don't throw books.\",\n",
       " 'the+end.',\n",
       " 'all done?',\n",
       " 'all done?',\n",
       " 'yeah?',\n",
       " 'hmm?',\n",
       " \"what's you say?\",\n",
       " \"that's it.\",\n",
       " 'we read them.',\n",
       " 'you want to read this one again?',\n",
       " 'ok.',\n",
       " 'Ten_Little_Fingers_And_Ten_Little_Toes.',\n",
       " 'there was one little baby who was born far away and another who was born on the very next day.',\n",
       " 'and both of these babies as everyone knows, had ten little fingers and ten little toes.',\n",
       " 'there was one little baby who was born in a town and another who was wrapped in an eiderdown.',\n",
       " 'and both of these babies as everyone knows -- had ten little fingers and ten little toes.',\n",
       " 'there was one little baby who was born in the hills and another who suffered from sneezes and chills.',\n",
       " 'chills?',\n",
       " \"I think it's cold.\",\n",
       " 'and both of these babies as everyone knows --',\n",
       " 'put that in your mouth.',\n",
       " 'had ten little fingers --',\n",
       " 'and ten little toes.',\n",
       " 'yep.',\n",
       " 'yep, toes on your feet.',\n",
       " 'there was one little baby who was born on the ice -- and another in a tent who was just as nice.',\n",
       " 'and both of these babies as everyone knows -- had ten little fingers and ten little toes.',\n",
       " 'spit it out.',\n",
       " \"don't take it out.\",\n",
       " 'but the next baby -- born was truly divine.',\n",
       " 'a sweet little child who was mine, all mine.',\n",
       " \"D'Angelo@n, mine all mine.\",\n",
       " 'and this little baby as everyone knows -- has ten little fingers --',\n",
       " \"where's your ten fingers?\",\n",
       " 'show me -- show me your ten fingers.',\n",
       " 'ten fingers.',\n",
       " 'and ten little toes.',\n",
       " \"you don't have to be scared.\",\n",
       " \"it's just pictures.\",\n",
       " \"ok, we won't read that one again, ok?\",\n",
       " 'and three little kisses -- on the tip of its nose.',\n",
       " 'thank+you.',\n",
       " 'the+end.',\n",
       " 'the+end.',\n",
       " 'stop that.',\n",
       " 'where you going?',\n",
       " 'are you done?',\n",
       " 'you done?',\n",
       " 'already?',\n",
       " \"that's it?\",\n",
       " \"that's it?\",\n",
       " \"that's it?\",\n",
       " 'you done?',\n",
       " 'hey, no -- no -- no.',\n",
       " 'no -- no.',\n",
       " 'come here.',\n",
       " 'here.',\n",
       " 'come here ###',\n",
       " \"no, D'Angelo@n.\",\n",
       " '---']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LabelBinarizer = preprocessing.LabelBinarizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multioutput target data is not supported with label binarization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-3b1345adb10e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m             ])\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mSVC_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m#SVC_pipeline.fit(text_list.LabelBinarizer(), classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;31m# overall.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mShape\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mproblems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \"\"\"\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_type_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'multioutput'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_type_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m             raise ValueError(\"Multioutput target data is not supported with \"\n\u001b[0m\u001b[0;32m    432\u001b[0m                              \"label binarization\")\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Multioutput target data is not supported with label binarization"
     ]
    }
   ],
   "source": [
    "#  Sequentially apply a list of transforms and a final estimator \n",
    "# add your stop_words  (stop_words)\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "SVC_pipeline.fit(text_list, classes)\n",
    "#SVC_pipeline.fit(text_list.LabelBinarizer(), classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new path\n",
    "# K:\\New Reorganization\\Research\\Research Management\\Data Manager Materials\\pickled_models\\archived spatial models\n",
    "pickle.dump(SVC_pipeline,open('C:/Users/das2/Documents/spa_SVC_063.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training ends and model runs\n",
    "# pass the english models\n",
    "\n",
    "obj = pickle.load(open('C:/Users/das2/Documents/spa_SVC_063.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-24a134ec0657>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# test prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Number'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'two'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'baby'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1244\u001b[0m                 \u001b[1;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m                 \"string object received.\")\n\u001b[1;32m-> 1246\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "# test prediction\n",
    "obj.predict(['Number', 'two', 'baby'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-9acca95507ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Number'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'two'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'baby'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"speech\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# append to list here # check the header names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1244\u001b[0m                 \u001b[1;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m                 \"string object received.\")\n\u001b[1;32m-> 1246\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for df in li:\n",
    "    x = obj.predict(df[\"speech\"].tolist())\n",
    "    # append to list here # check the header names\n",
    "    new.append(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
